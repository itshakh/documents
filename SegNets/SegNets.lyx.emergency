#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
SegNets Overview, Sep 2018
\end_layout

\begin_layout Author
Tzachi Hershkovich
\end_layout

\begin_layout Section*
Papers summaries
\end_layout

\begin_layout Subsection*
ENet, 2016
\end_layout

\begin_layout Standard
ENet is a lightweight segmentation network that was orignally design got
 real time applications which requires semantic segmentation in high frequencies.
\end_layout

\begin_layout Standard
Basic architecture of the network can be seen in fig.
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ENet-architecuture-:"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Enet/architecture_table.png
	lyxscale 50
	scale 30

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Enet/inital_layer.png
	scale 30

\end_inset


\begin_inset Graphics
	filename Enet/bottleneck.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:ENet-architecuture-:"

\end_inset

ENet architecuture : each bottleneck section is design according to (b).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Section 4 - Design choises, feature map resolution.

\series default
 
\begin_inset Quotes erd
\end_inset

...
 However, downsampling has one big advantage.
 Filters operating on downsampled images have a bigger receptive field,
 that allows them to gather more context.
 This is especially important when trying to differentiate between classes
 like, for example, rider and pedestrian in a road scene.
 It is not enough that the network learns how people look, the context in
 which they appear is equally important.
 In the end, we have found that it is better to use dilated convolutions
 for this purpose [30].
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Section 4 - Decoder size.
 
\series default

\begin_inset Quotes eld
\end_inset

In this work we would like to provide a different view on encoder-decoder
 architec- tures than the one presented in [11].
 SegNet is a very symmetric architecture, as the encoder is an exact mirror
 of the encoder.
 Instead, our architecture consists of a large encoder, and a small decoder.
 This is motivated by the idea that the encoder should be able to work in
 a similar fashion to original classification architectures, i.e.
 to operate on smaller resolution data and provide for information processing
 and filtering.
 Instead, the role of the the decoder, is to upsample the output of the
 encoder, only fine-tuning the details.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Section 4 - Nonlinear operations.
 
\series default

\begin_inset Quotes eld
\end_inset

...We replaced all ReLUs in the network with PReLUs [26], which use an additional
 parameter per feature map, with the goal of learning the negative slope
 of non-linearities.
 We expected that in layers where identity is a preferable transfer function,
 PReLU weights will have values close to 1, and conversely, values around
 0 if ReLU is preferable.
 Results of this experiment can be seen in fig.
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:PReLU-weight-distribution"

\end_inset

.
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Enet/Prelu.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:PReLU-weight-distribution"

\end_inset

PReLU weight distribution vs network depth.
 Blue line is the weights mean, while an area between maximum and minimum
 weight is grayed out.
 Each vertical dotted line corresponds to a PReLU in the main branch and
 marks the boundary between each of bottleneck blocks.
 The gray vertical line at 67th module is placed at encoder-decoder border.
\end_layout

\end_inset


\end_layout

\end_inset

 It is notable that the decoder weights become much more positive and learn
 functions closer to identity.
 This confirms our intuitions that the decoder is used only to fine-tune
 the upsampled output.
\end_layout

\begin_layout Itemize

\series bold
Section 4 - Information-preserving dimensionality changes.
 
\series default

\begin_inset Quotes eld
\end_inset

...we chose to perform pooling operation in parallel with a convolution of
 stride 2, and concatenate resulting feature maps.
 This technique allowed us to speed up inference time of the initial block
 10 times.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Section 4 - Factorizing filters.
 
\series default
It has been shown that convolutional weights have a fair amount of redundancy,
 and each n × n convolution can be decomposed into two smaller ones following
 each other: one with a n × 1 filter and the other with a 1 × n filter [32].
 This idea has been also presented in [28], and from now on we adopt their
 naming convention and will refer to these as asymmetric convolutions.
 We have used asymmetric convolutions with n = 5 in our network, so cost
 of these two operations is similar to a single 3 × 3 convolution.
 This allowed to increase the variety of functions learned by blocks and
 increase the receptive field.
\end_layout

\begin_layout Itemize

\series bold
Section 5 - Results.
 
\begin_inset Graphics
	filename Enet/Runtime_comparisson_1.png

\end_inset


\end_layout

\begin_layout Standard

\series bold
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
